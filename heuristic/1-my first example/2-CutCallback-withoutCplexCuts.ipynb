{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint\n",
    "from math import floor, fabs\n",
    "from cplex.callbacks import UserCutCallback\n",
    "import cplex  as CPX\n",
    "import cplex.callbacks as CPX_CB\n",
    "import sys\n",
    "\n",
    "def SecModelParameter ( VarsValue, m, n, my_rhs, my_coef, BranchedVariables):\n",
    "#     print 'BranchedVariables',BranchedVariables\n",
    "    unbranched=list()\n",
    "    c=[]\n",
    "    d=(np.zeros(((2 * m) + (4 * n), 2*n)))\n",
    "    halatest=[]\n",
    "    a=[]\n",
    "    newrow=[]\n",
    "    zer=(np.zeros(((2 * m) + (4 * n), 2*n)))\n",
    "    for i in range (n):\n",
    "        if i not in (BranchedVariables):\n",
    "            unbranched.append(i)\n",
    "    NodeNum=n-len(unbranched)\n",
    "    \n",
    "    \n",
    "    for i in unbranched:   \n",
    "        for h in range (m):\n",
    "            c.append(0)\n",
    "        for h in range (m):\n",
    "            lin=0            \n",
    "            for j in range (n):\n",
    "#                 print 'i', i, 'h', h,'j',j\n",
    "#                 print 'j1',j\n",
    "                if j in (BranchedVariables):\n",
    "#                     print 'j2',j\n",
    "                    sv = VarsValue[BranchedVariables.index(j)]\n",
    "#                     print 'sv', sv\n",
    "                    lin =lin+ (sv * my_coef[h][j]) \n",
    "#                     print 'lin', lin\n",
    "#                 print 'sv'\n",
    "            lin=lin- my_rhs[h]\n",
    "            c.append(lin)                \n",
    "        for h in range (n):\n",
    "            c.append(0)\n",
    "        for h in range (n):\n",
    "            c.append(0)\n",
    "\n",
    "        for jj in range (n):\n",
    "            lin2=-1\n",
    "            if jj in (BranchedVariables):\n",
    "                sv = VarsValue[BranchedVariables.index(jj)]\n",
    "                lin2 =lin2+ sv\n",
    "            c.append(lin2)\n",
    "            \n",
    "        for jj in range (n):\n",
    "            lin2=0\n",
    "            if jj in (BranchedVariables):\n",
    "                lin2 =-1*VarsValue[BranchedVariables.index(jj)]\n",
    "            c.append(lin2)\n",
    "\n",
    "              \n",
    "    k=NodeNum-1 \n",
    "\n",
    "    for i in unbranched: \n",
    "        k=k+1\n",
    "        for t in range(m):              \n",
    "            for j in range(n):    \n",
    "                d[(k-NodeNum)*(2*m+4*n)+t][j+n]=my_coef[t][j] \n",
    "                \n",
    "        for t in range(m):  \n",
    "            d[(k-NodeNum)*(2*m+4*n)+t][n+i]=d[(k-NodeNum)*(2*m+4*n)+t][n+i]-my_rhs[t]   \n",
    "                \n",
    "        for t in range(m):  \n",
    "            for kk in unbranched:\n",
    "                d[(k-NodeNum)*(2*m+4*n)+t+m][kk]=my_coef[t][kk] \n",
    "                \n",
    "\n",
    "        for t in range(m):  \n",
    "            for j in range(n):\n",
    "                d[(k-NodeNum)*(2*m+4*n)+t+m][j+n]=-1*my_coef[t][j] \n",
    "                 \n",
    "        for t in range(m):  \n",
    "            d[(k-NodeNum)*(2*m+4*n)+t+m][n+i]=d[(k-NodeNum)*(2*m+4*n)+t+m][n+i]+my_rhs[t] \n",
    "                \n",
    "        for t in range(n):              \n",
    "            d[(k-NodeNum)*(2*m+4*n)+t+2*m][i]=-1 \n",
    "            \n",
    "        for t in range(n):  \n",
    "            \n",
    "            d[(k-NodeNum)*(2*m+4*n)+2*m+t][n+t]=1 \n",
    "        \n",
    "            \n",
    "        for t in range(n):  \n",
    "            \n",
    "            d[(k-NodeNum)*(2*m+4*n)+2*m+n+t][n+t]=-1 \n",
    "        \n",
    "        for mm in unbranched: \n",
    "             \n",
    "            d[(k-NodeNum)*(2*m+4*n)+2*m+2*n+mm][mm]=1  \n",
    "            \n",
    "        for t in range(n):  \n",
    "            \n",
    "            d[(k-NodeNum)*(2*m+4*n)+t+2*n+2*m][i]=d[(k-NodeNum)*(2*m+4*n)+t+2*n+2*m][i]+1            \n",
    "            \n",
    "\n",
    "        for t in range(n):  \n",
    "            \n",
    "            d[(k-NodeNum)*(2*m+4*n)+2*m+2*n+t][n+t]=-1 \n",
    "\n",
    "        for t in unbranched:  \n",
    "            d[(k-NodeNum)*(2*m+4*n)+2*m+3*n+t][t]=-1  \n",
    "               \n",
    "        \n",
    "        for t in range(n):  \n",
    "            \n",
    "            d[(k-NodeNum)*(2*m+4*n)+2*m+3*n+t][n+t]=1 \n",
    "            \n",
    "        d=np.vstack([d, zer])                          \n",
    "            \n",
    "    d = np.delete(d, slice(((n - NodeNum)*((2 * m) + (4 * n))),((n-NodeNum+1)*((2 * m) + (4 * n)))), axis=0)\n",
    "#     print 'c',c\n",
    "#     print 'unbranched',unbranched\n",
    "    return c, d,unbranched\n",
    "#     print c, d,unbranched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SecondModel (cc,dd,NotBranched):\n",
    "#     print 'cc,dd,NotBranched',cc,dd,NotBranched\n",
    "    numrows = len(dd)    # 3 rows in your example\n",
    "    numcols = len(dd[0])\n",
    "    VarNum=numcols/2\n",
    "    ConstNum=numrows/len(NotBranched)\n",
    "    my_obj=[]\n",
    "    my_ub=[]\n",
    "    my_lb=[]\n",
    "    my_colnames=[]\n",
    "    my_rownames=[\"c1\"]\n",
    "    my_coef=[] \n",
    "    my_row=[]\n",
    "    my_rhs=[]\n",
    "    my_sense=\"\"\n",
    "    my_Nmah=list()\n",
    "    \n",
    "    for i in range(numrows):\n",
    "        my_obj=cc\n",
    "        my_ub.append(1)\n",
    "        my_lb.append(0)\n",
    "        my_colnames.append(\"y\"+str(i))\n",
    "\n",
    "    sumarray=[]     \n",
    "    NotBranched.sort()    \n",
    "#     sumshode=list()\n",
    "    for t in range (VarNum):\n",
    "        sumlist=list()\n",
    "        for j in range(numrows):\n",
    "#             print 't,j',t,j\n",
    "            if t in NotBranched: \n",
    "                indj=NotBranched.index(t)\n",
    "#                 print 'indj',indj\n",
    "                indjrange=range(indj*ConstNum,(indj+1)*ConstNum)\n",
    "#                 print 'indjrange',indjrange\n",
    "                if j in indjrange:\n",
    "#                     print 'dd[j,t]+dd[j,t+VarNum]',j,'ll',dd[j,t]+dd[j,t+VarNum]\n",
    "                    sumlist.append(dd[j,t]+dd[j,t+VarNum])\n",
    "                    dd[j,t+VarNum]=0\n",
    "                else:\n",
    "                    sumlist.append(dd[j,t])\n",
    "            else:\n",
    "                sumlist.append(dd[j,t])\n",
    "        sumarray.append(sumlist)\n",
    "#     print 'sumarray',sumarray\n",
    "#     print 'newDD',dd\n",
    "  \n",
    "#     print 'notbranched',NotBranched\n",
    "    lenNotbranched=len(NotBranched)\n",
    "   \n",
    "    \n",
    "    for t in range(lenNotbranched):  \n",
    "        for i in range (VarNum):\n",
    "            sumlist=list()\n",
    "            if t>0:\n",
    "                for p in range(t):\n",
    "                    for j in range (ConstNum):\n",
    "                        sumlist.append(0) \n",
    "                        \n",
    "            for j in range(ConstNum):\n",
    "                sumlist.append(dd[t*ConstNum+j,i+VarNum])\n",
    "            if i in NotBranched and i > NotBranched[t]:\n",
    "                indx=NotBranched.index(i)\n",
    "                indy=NotBranched.index(NotBranched[t])\n",
    "                \n",
    "                for x in range(indx-indy-1):\n",
    "                    for j in range (ConstNum):\n",
    "                        sumlist.append(0) \n",
    "                for j in range(ConstNum):\n",
    "                    sumlist.append(dd[(indx)*ConstNum+j,NotBranched[t]+VarNum])\n",
    "                    dd[(indx)*ConstNum+j,NotBranched[t]+VarNum]=0\n",
    "#                 sumarray.append([t+indx,indy])\n",
    "                if t<lenNotbranched-2 :\n",
    "                    for p in range(lenNotbranched-indx-1):\n",
    "                        for j in range (ConstNum):\n",
    "                            sumlist.append(0)\n",
    "            else:\n",
    "                if t<lenNotbranched-1 :\n",
    "#                     print 'lenNotbranched',lenNotbranched\n",
    "                    for p in range(lenNotbranched-t-1):\n",
    "                            for j in range (ConstNum):\n",
    "                                sumlist.append(0) \n",
    "#             print 't,i',t,',',i\n",
    "#             print 'sumlist p2',sumlist\n",
    "#             print 'lensumlist p2',len(sumlist)\n",
    "            sumarray.append(sumlist)\n",
    "#     print 'part2' ,sumarray  \n",
    "    \n",
    "    rows=[]\n",
    "    for j in range(numrows):\n",
    "        my_row.append(1)\n",
    "    rows.append([range(numrows),my_row])  \n",
    "    def populatebyrow(SecProb):\n",
    "        SecProb.objective.set_sense(SecProb.objective.sense.maximize)\n",
    "        SecProb.variables.add(obj = my_obj,lb=my_lb, names = my_colnames)\n",
    "        \n",
    "        for i in range (len(sumarray)):\n",
    "            if all(k == 0 for k in sumarray[i])!=True:\n",
    "#                 print 'i',i\n",
    "                sumline=[]\n",
    "                sumline.append([range(len(sumarray[i])),sumarray[i]])\n",
    "#                 print 'sumline',sumline\n",
    "                SecProb.linear_constraints.add(lin_expr = sumline, senses =  \"E\",\n",
    "                                rhs = [0], names = [\"u\"+str(i)])\n",
    "        SecProb.linear_constraints.add(lin_expr = rows, senses = \"L\",\n",
    "                                             rhs = [1], names = [\"t0\"])\n",
    "    \n",
    "    my_SecProb = CPX.Cplex()\n",
    "    handle = populatebyrow(my_SecProb)\n",
    "    my_SecProb.solve()\n",
    "    my_SecProb.write(\"lpex2.lp\")\n",
    "    SecVariables    = my_SecProb.solution.get_values()\n",
    "    SecObjective=my_SecProb.solution.get_objective_value()\n",
    "#     print 'SecVariables,SecObjective,my_colnames',SecVariables, SecObjective,my_colnames\n",
    "    return SecVariables, SecObjective,my_colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GeneratintCutParameters ( m, n, my_rhs, my_coef, BranchedVariables,SecVariables): \n",
    "    BranchedLen=len(BranchedVariables)\n",
    "    unbranched=list()\n",
    "    for i in range (n):\n",
    "        if i not in (BranchedVariables):\n",
    "            unbranched.append(i)  \n",
    "    LeUnbranched=len(unbranched)                   \n",
    "#     NodeNum=n-len(unbranched)\n",
    "#     k=NodeNum-1 \n",
    "    lhs=(np.zeros((LeUnbranched*((2 * m) + (4 * n)),BranchedLen)))\n",
    "    rhs=(np.zeros((LeUnbranched*((2 * m) + (4 * n)),1)))\n",
    "\n",
    "    for i in range (LeUnbranched):   \n",
    "        for h in range (m):\n",
    "            for j in (BranchedVariables):\n",
    "                lhs[i*(2*m+4*n)+m+h][BranchedVariables.index(j)]=my_coef[h][j]*SecVariables[i*(2*m+4*n)+m+h]  \n",
    "                rhs[i*(2*m+4*n)+m+h]=my_rhs[h]*SecVariables[i*(2*m+4*n)+m+h]  \n",
    "            \n",
    "        for h in range (n):\n",
    "            for j in (BranchedVariables):\n",
    "                lhs[i*(2*m+4*n)+2*m+2*n+j][BranchedVariables.index(j)]=SecVariables[i*(2*m+4*n)+2*m+2*n+j]  \n",
    "                rhs[i*(2*m+4*n)+2*m+2*n+h]=SecVariables[i*(2*m+4*n)+2*m+2*n+h]  \n",
    "            \n",
    "#         for h in range (n):\n",
    "        for j in (BranchedVariables):\n",
    "            lhs[i*(2*m+4*n)+2*m+3*n+j][BranchedVariables.index(j)]=-1*SecVariables[i*(2*m+4*n)+2*m+3*n+j]  \n",
    "\n",
    "    #     print 'lefttt',lhs,'rightttt', rhs\n",
    "    LenLeft=len(lhs)\n",
    "    LenRight=len(rhs)\n",
    "    LenSec=len(SecVariables)\n",
    "#     print 'LenLeft',LenLeft\n",
    "#     print'LenRight',LenRight\n",
    "#     print 'LenSec',LenSec\n",
    "#     indicesLhs=[]\n",
    "#     for j in range (8):\n",
    "#         indicesLhs.append([i for i in range(LenLeft) if lhs[i][j]!=0]) \n",
    "#     print indicesLhs\n",
    "#     indicesRhs = [(i,rhs[i]) for i in range(LenRight) if rhs[i]>0]\n",
    "#     print 'indicesRhs',indicesRhs\n",
    "#     indiceSecVariables=[(i , SecVariables[i]) for i in range(LenSec) if SecVariables[i]!=0]\n",
    "#     ValueSecVariables=[SecVariables[i] for i in range(LenSec) if SecVariables[i]!=0]\n",
    "#     print 'positiveLeftHand',indicesLhs \n",
    "#     print 'indiceSecVariables',indiceSecVariables\n",
    "#     print 'ValueSecVariables', ValueSecVariables\n",
    "    FinalLhs=np.sum(lhs, axis=0)\n",
    "    FinalRhs=np.sum(rhs, axis=0)\n",
    "    print 'FinalLhs',FinalLhs, 'FinalRhs',FinalRhs\n",
    "    return FinalLhs,FinalRhs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCut(CPX_CB.UserCutCallback):\n",
    "\n",
    "    def __call__(self):\n",
    "         \n",
    "        self.times_called += 1\n",
    "        print  'self.times_called', self.times_called\n",
    "        branched=list()\n",
    "        values=list()\n",
    "        branchedNames=list()\n",
    "        global BranchedMatrix\n",
    "        senses= \"L\"\n",
    "#         print 'self.get_lower_bounds(s)',self.get_lower_bounds()\n",
    "#         print 'self.get_upper_bounds(s)',self.get_upper_bounds()\n",
    "#         print 'objective value', self.get_objective_value()\n",
    "#         print 'values', self.get_values()\n",
    "        print 'get_num_nodes', self.get_num_nodes()\n",
    "\n",
    "        lb=self.get_lower_bounds()\n",
    "        ub=self.get_upper_bounds()               \n",
    "        for j in range(len(lb)):\n",
    "            if lb[j]==ub[j]:\n",
    "                branched.append(j)\n",
    "                values.append(lb[j])\n",
    "                branchedNames.append(VarsName[j])\n",
    "                \n",
    "#         print 'values',values\n",
    "        print 'branchedNames',branchedNames\n",
    "#         print 'branched',branched\n",
    "\n",
    "        if len(values)>0:  \n",
    "            BranchedMatrix.append([branched,values])\n",
    "#             BranchedValue.append(values)\n",
    "#             print 'BranchedMatrix',BranchedMatrix\n",
    "\n",
    "            if [BranchedMatrix[-1]]!=[BranchedMatrix[-2]] :\n",
    "                SecModelParameterOutput=SecModelParameter(values, self.m, self.n, self.my_rhs, self.my_coef,branched)\n",
    "                SecondModelOutput=SecondModel (SecModelParameterOutput[0],SecModelParameterOutput[1],SecModelParameterOutput[2])\n",
    "#                 print 'SecondModelOutput[1]',SecondModelOutput[1]\n",
    "#                 print 'SecondModelOutput[0]',SecondModelOutput[0]\n",
    "                if SecondModelOutput[1]>0.000001:\n",
    "    #                 print 'bayad cut bezane'\n",
    "                    output=GeneratintCutParameters (self.m, self.n, self.my_rhs, self.my_coef,branched, SecondModelOutput[0])\n",
    "#                     print 'output',output[0]\n",
    "#                     print 'output[1]',output[1]\n",
    "                    lhs=[CPX.SparsePair(ind =branchedNames, val = output[0])]\n",
    "#                     print 'lhs',lhs[0]\n",
    "                    self.add(cut=lhs[0],rhs=output[1][0],sense=senses)\n",
    "#                     print 'cut added'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.times_called 1\n",
      "get_num_nodes 0\n",
      "values [1.0]\n",
      "branchedNames ['x13']\n",
      "branched [13]\n",
      "CPXPARAM_Read_DataCheck                          1\n",
      "Tried aggregator 1 time.\n",
      "LP Presolve eliminated 0 rows and 392 columns.\n",
      "Reduced LP has 120 rows, 868 columns, and 10737 nonzeros.\n",
      "Presolve time = 0.02 sec. (1.17 ticks)\n",
      "Initializing dual steep norms . . .\n",
      "\n",
      "Iteration log . . .\n",
      "Iteration:     1   Scaled dual infeas =           103.775495\n",
      "Iteration:    62   Scaled dual infeas =             9.019207\n",
      "Iteration:   124   Scaled dual infeas =             0.562417\n",
      "Iteration:   186   Scaled dual infeas =             0.176057\n",
      "Iteration:   245   Dual objective     =             0.005766\n",
      "SecondModelOutput[1] 0.0\n",
      "SecondModelOutput[0] [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "self.times_called 2\n",
      "get_num_nodes 0\n",
      "values [1.0]\n",
      "branchedNames ['x13']\n",
      "branched [13]\n",
      "self.times_called 3\n",
      "get_num_nodes 1\n",
      "values [1.0, 1.0]\n",
      "branchedNames ['x11', 'x13']\n",
      "branched [11, 13]\n",
      "CPXPARAM_Read_DataCheck                          1\n",
      "Tried aggregator 1 time.\n",
      "LP Presolve eliminated 0 rows and 338 columns.\n",
      "Reduced LP has 118 rows, 832 columns, and 9827 nonzeros.\n",
      "Presolve time = 0.00 sec. (1.07 ticks)\n",
      "Initializing dual steep norms . . .\n",
      "\n",
      "Iteration log . . .\n",
      "Iteration:     1   Scaled dual infeas =           725.380926\n",
      "Iteration:    62   Scaled dual infeas =            65.772404\n",
      "Iteration:   124   Scaled dual infeas =             0.645056\n",
      "Iteration:   186   Scaled dual infeas =             0.438072\n",
      "Iteration:   248   Scaled dual infeas =             0.007104\n",
      "Iteration:   257   Dual objective     =             0.018512\n",
      "SecondModelOutput[1] 0.0\n",
      "SecondModelOutput[0] [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "self.times_called 4\n",
      "get_num_nodes 1\n",
      "values [1.0, 1.0]\n",
      "branchedNames ['x11', 'x13']\n",
      "branched [11, 13]\n",
      "self.times_called 5\n",
      "get_num_nodes 2\n",
      "values [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "branchedNames ['x0', 'x2', 'x5', 'x11', 'x13']\n",
      "branched [0, 2, 5, 11, 13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPXPARAM_Read_DataCheck                          1\n",
      "Tried aggregator 1 time.\n",
      "LP Presolve eliminated 0 rows and 200 columns.\n",
      "Reduced LP has 106 rows, 700 columns, and 7229 nonzeros.\n",
      "Presolve time = 0.02 sec. (0.79 ticks)\n",
      "Initializing dual steep norms . . .\n",
      "\n",
      "Iteration log . . .\n",
      "Iteration:     1   Scaled dual infeas =            55.357141\n",
      "Iteration:    62   Scaled dual infeas =             0.207357\n",
      "Perturbation started.\n",
      "Iteration:   102   Scaled dual infeas =             0.207372\n",
      "Iteration:   164   Scaled dual infeas =             0.010676\n",
      "Iteration:   169   Dual objective     =             0.024130\n",
      "Iteration:   230   Dual objective     =             0.011148\n",
      "Iteration:   292   Dual objective     =             0.000002\n",
      "Removing perturbation.\n",
      "SecondModelOutput[1] 0.0\n",
      "SecondModelOutput[0] [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "self.times_called 6\n",
      "get_num_nodes 2\n",
      "values [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "branchedNames ['x0', 'x2', 'x5', 'x11', 'x13']\n",
      "branched [0, 2, 5, 11, 13]\n",
      "self.times_called 7\n",
      "get_num_nodes 3\n",
      "values [-0.0, 1.0, 1.0, 1.0, 1.0]\n",
      "branchedNames ['x0', 'x2', 'x5', 'x11', 'x13']\n",
      "branched [0, 2, 5, 11, 13]\n",
      "CPXPARAM_Read_DataCheck                          1\n",
      "Tried aggregator 1 time.\n",
      "LP Presolve eliminated 0 rows and 200 columns.\n",
      "Reduced LP has 106 rows, 700 columns, and 7229 nonzeros.\n",
      "Presolve time = 0.00 sec. (0.79 ticks)\n",
      "Initializing dual steep norms . . .\n",
      "\n",
      "Iteration log . . .\n",
      "Iteration:     1   Scaled dual infeas =            27.499998\n",
      "Iteration:    62   Scaled dual infeas =             0.172748\n",
      "Iteration:   124   Scaled dual infeas =             0.078737\n",
      "Iteration:   159   Dual objective     =             0.016618\n",
      "Iteration:   220   Dual objective     =             0.005687\n",
      "Iteration:   282   Dual objective     =             0.001087\n",
      "Removing shift (1).\n",
      "SecondModelOutput[1] 0.0\n",
      "SecondModelOutput[0] [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "self.times_called 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_num_nodes 3\n",
      "values [-0.0, 1.0, 1.0, 1.0, 1.0]\n",
      "branchedNames ['x0', 'x2', 'x5', 'x11', 'x13']\n",
      "branched [0, 2, 5, 11, 13]\n",
      "self.times_called 9\n",
      "get_num_nodes 4\n",
      "values [-0.0, 1.0, -0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "branchedNames ['x0', 'x2', 'x4', 'x5', 'x8', 'x11', 'x12', 'x13', 'x14']\n",
      "branched [0, 2, 4, 5, 8, 11, 12, 13, 14]\n",
      "CPXPARAM_Read_DataCheck                          1\n",
      "Tried aggregator 1 time.\n",
      "LP Presolve eliminated 0 rows and 72 columns.\n",
      "Reduced LP has 76 rows, 468 columns, and 4073 nonzeros.\n",
      "Presolve time = 0.02 sec. (0.45 ticks)\n",
      "Initializing dual steep norms . . .\n",
      "\n",
      "Iteration log . . .\n",
      "Iteration:     1   Scaled dual infeas =           522.388227\n",
      "Iteration:    62   Scaled dual infeas =             3.598555\n",
      "Iteration:   124   Scaled dual infeas =             0.265800\n",
      "Iteration:   140   Dual objective     =             0.062512\n",
      "Iteration:   201   Dual objective     =             0.014521\n",
      "Iteration:   263   Dual objective     =             0.002045\n",
      "SecondModelOutput[1] 0.0013835229019\n",
      "SecondModelOutput[0] [0.0, 0.0, 6.688527334721759e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0002445954912674982, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0004177454258493211, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013575470585610101, 0.012096577405247297, 0.02953147278373589, 0.047602782527302365, 0.0, 0.0, 0.0, 0.0, 0.05283900045100491, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.179093108557297e-05, 0.0, 0.0, 0.03010020717903184, 0.0, 0.0, 0.04715335989799833, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00027306708378723593, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0006288853100187221, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00028083175117775845, 0.0, 0.0, 0.0, 0.00032491754026792256, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00011175720264456398, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008689641872016252, 0.0, 0.0, 0.0, 0.0, 0.0, 0.029268568327767322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023533913853743323, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0690992814611664, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.035923501706222924, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009446969027714159, 0.004875069495058049, 0.011769326418659687, 0.05255802659690367, 0.006195239088572914, 0.0, 0.0, 0.0, 0.008809707169924953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00011890289132310225, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004042698304985476, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011533580458340917, 0.0, 0.0, 0.0, 0.0048750185442471925, 0.0, 0.0, 0.007253076370709237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016250890950231814, 0.0, 0.0, 0.0, 0.0022591549351389426, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004280504087631681, 0.0, 0.007609785044678544, 0.002853669391754454, 0.0, 0.0, 0.0, 0.0, 0.009155522631878873, 0.0, 0.0, 0.015144212403395585, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.5642471086048366e-05, 0.0, 0.0, 0.0, 8.872542050713267e-05, 7.760145227527766e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003799059410818109, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.034927978573424584, 0.0038706391119082265, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016774019613758587, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0013216384430923206, 0.0, 0.0, 0.0079232173344142, 0.0, 0.0, 0.0032092227691820864, 0.0, 0.0, 0.0, 0.00037401818480431665, 0.0, 0.0069832392144419135, 0.0067995395582223245, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0761025213280086e-05, 0.0, 0.0, 0.0, 5.931721489508596e-06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0012871835632233654, 0.0, 0.0, 0.0013672618033317313, 0.0, 0.0013198080314156626, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0002876884922411669, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001293115284712874, 0.0, 0.0, 0.0, 0.0010811585125231052, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007533286291675917, 0.0, 0.001672745460041424, 0.0005486842377795451, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.432061225995745e-05, 0.00028348354879156304, 0.0005896164956059052, 0.0, 0.0, 0.0, 0.00027773004669934433, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04594268338890524, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027973497148700437, 0.029104151329689045, 0.0, 0.0, 0.0, 0.0, 0.0, 0.026095183110654563, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.025788104093724732, 0.0, 0.0, 0.0, 0.0, 0.0, 0.054865793229241774, 0.03813181012857667, 0.0028971130432394188, 0.07981096975833742, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "FinalLhs [-0.01482555  0.00607492 -0.05473109  0.06555707  0.02085943  0.07572104\n",
      "  0.06205244  0.03720748  0.19255988] FinalRhs [0.45864874]\n",
      "output [-0.01482555  0.00607492 -0.05473109  0.06555707  0.02085943  0.07572104\n",
      "  0.06205244  0.03720748  0.19255988]\n",
      "output[1] [0.45864874]\n",
      "lhs SparsePair(ind = ['x0', 'x2', 'x4', 'x5', 'x8', 'x11', 'x12', 'x13', 'x14'], val = array([-0.01482555,  0.00607492, -0.05473109,  0.06555707,  0.02085943,\n",
      "        0.07572104,  0.06205244,  0.03720748,  0.19255988]))\n",
      "cut added\n",
      "self.times_called 10\n",
      "get_num_nodes 6\n",
      "values [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "branchedNames ['x0', 'x2', 'x5', 'x11', 'x13', 'x14']\n",
      "branched [0, 2, 5, 11, 13, 14]\n",
      "CPXPARAM_Read_DataCheck                          1\n",
      "Tried aggregator 1 time.\n",
      "LP Presolve eliminated 0 rows and 162 columns.\n",
      "Reduced LP has 100 rows, 648 columns, and 6407 nonzeros.\n",
      "Presolve time = 0.02 sec. (0.70 ticks)\n",
      "Initializing dual steep norms . . .\n",
      "\n",
      "Iteration log . . .\n",
      "Iteration:     1   Scaled dual infeas =          1310.192768\n",
      "Iteration:    62   Scaled dual infeas =            90.232198\n",
      "Iteration:   124   Scaled dual infeas =             2.148213\n",
      "Iteration:   186   Scaled dual infeas =             0.925403\n",
      "Iteration:   248   Scaled dual infeas =             0.021915\n",
      "Iteration:   260   Dual objective     =             0.027732\n",
      "Iteration:   321   Dual objective     =             0.012883\n",
      "Iteration:   383   Dual objective     =             0.004873\n",
      "Iteration:   445   Dual objective     =             0.002334\n",
      "SecondModelOutput[1] 0.00206671365035\n",
      "SecondModelOutput[0] [0.0, 0.0, 7.672229252382429e-07, 2.2319212370567068e-06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00021370645844817968, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01032918545978012, 0.0, 0.0, 0.0, 0.0, 9.639109842538653e-05, 0.0, 0.0, 0.0, 0.0, 2.17612320613029e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0002475340146973204, 0.0, 0.0, 0.00022667950063857178, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00017076724569365254, 0.0, 0.0, 0.0, 0.0, 9.79392629372041e-06, 4.799944205955971e-05, 0.0, 4.1261607250414094e-05, 0.0, 0.00013264403311779273, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0023313689681154167, 0.0, 0.0, 0.0, 0.0, 0.007308487682406816, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0005057491041079515, 0.0, 0.0, 0.0, 0.0, 0.01246025328124347, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008746456070141628, 0.014879324891546704, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.000122234916867017, 0.0001792006290308563, 0.0, 0.0, 0.0, 3.8591773320713763e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01637000931522222, 0.0, 0.0, 0.0, 0.014109041707358787, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004315852101707536, 0.0, 0.0, 0.0, 0.0, 0.009136753370895812, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019766323201363862, 0.0, 0.0, 0.011514927645616112, 0.0, 0.0, 0.0, 0.0, 0.0018635142695268337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02310432473293201, 0.0, 0.0, 0.023674068799660677, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00020902862973641557, 0.00020413028005251117, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020202301839173743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03472004347132156, 0.0, 0.0, 0.02823035925012487, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020656345082774863, 0.0, 0.0, 0.005785825143716448, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013330449212963027, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02731572229430277, 0.0, 0.0, 0.0, 0.0, 9.629834113636304e-06, 1.11952193669127e-05, 0.0, 7.057985184981104e-05, 0.0, 7.358021748036459e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.02580158276443e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.064786107246148e-05, 0.0, 0.0, 0.0, 0.0, 0.0019943197685627456, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0019530552385731252, 0.0, 0.0, 0.0, 0.0, 0.00829752255763544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0184436696176007, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013622751343091398, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012607222648115078, 0.003640841997703105, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009261736953180456, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.165851643830429e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.7208276290720254e-05, 0.0001423135781789294, 0.0, 0.0, 0.0, 4.613283613773197e-05, 0.0, 0.0, 3.4561680604236994e-05, 0.0, 0.0, 0.0, 0.0, 0.0031940840784016743, 0.0, 0.0, 0.0, 0.0, 0.0020827173650483496, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014056576390320215, 0.0, 0.0, 0.017935630815309606, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010465310382149701, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008133178010160542, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6.595199396445202e-05, 0.00047797651446753364, 0.00030668620802619265, 0.0, 0.00010865639908384076, 0.0, 0.00018967718936151157, 0.0, 0.0, 0.00040273346422285315, 0.0, 0.0, 0.00021572923351293838, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04578216006892485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.026049849317479716, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05132125163228997, 0.0, 0.01160481072607692, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04279543252157482, 0.0, 0.0, 0.11958249278467466, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.5309105394409884e-05, 0.0001682705877794792, 0.0, 8.904638858600912e-06, 0.0, 0.00023742201914514276, 0.0, 0.0, 9.056584401411181e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0033411025866748045, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027636022733224595, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022165697380047814, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014974537144898369, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004888972281886198, 0.0, 0.014355313768988721, 0.037253593467697174, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0001509756447947274, 0.00019963027077954566, 0.0, 0.0, 0.0, 2.9289651163713306e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018806861852453026, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016175127039496265, 0.0, 0.0, 0.0, 0.0, 0.006763566569184005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006297068801444607, 0.0, 0.0, 0.0, 0.0, 0.01446647774878572, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027752581518607676, 0.0, 0.0, 0.02639150740994923, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "FinalLhs [0.10963997 0.01332372 0.03675935 0.15360561 0.0326753  0.28076915] FinalRhs [0.62470638]\n",
      "output [0.10963997 0.01332372 0.03675935 0.15360561 0.0326753  0.28076915]\n",
      "output[1] [0.62470638]\n",
      "lhs SparsePair(ind = ['x0', 'x2', 'x5', 'x11', 'x13', 'x14'], val = array([0.10963997, 0.01332372, 0.03675935, 0.15360561, 0.0326753 ,\n",
      "       0.28076915]))\n",
      "cut added\n",
      "self.times_called 11\n",
      "get_num_nodes 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values [-0.0, 1.0]\n",
      "branchedNames ['x11', 'x13']\n",
      "branched [11, 13]\n",
      "CPXPARAM_Read_DataCheck                          1\n",
      "Tried aggregator 1 time.\n",
      "LP Presolve eliminated 0 rows and 338 columns.\n",
      "Reduced LP has 118 rows, 832 columns, and 9827 nonzeros.\n",
      "Presolve time = 0.00 sec. (1.07 ticks)\n",
      "Initializing dual steep norms . . .\n",
      "\n",
      "Iteration log . . .\n",
      "Iteration:     1   Scaled dual infeas =            99.265292\n",
      "Iteration:    62   Scaled dual infeas =             0.752129\n",
      "Iteration:   124   Scaled dual infeas =             0.691551\n",
      "Iteration:   186   Scaled dual infeas =             0.035979\n",
      "Iteration:   200   Dual objective     =             0.013254\n",
      "Iteration:   261   Dual objective     =             0.001722\n",
      "Removing shift (1).\n",
      "SecondModelOutput[1] 0.0\n",
      "SecondModelOutput[0] [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "self.times_called 12\n",
      "get_num_nodes 7\n",
      "values [-0.0, 1.0]\n",
      "branchedNames ['x11', 'x13']\n",
      "branched [11, 13]\n",
      "self.times_called 13\n",
      "get_num_nodes 8\n",
      "values [1.0, 1.0, 1.0, -0.0, -0.0, 1.0, 1.0, 1.0]\n",
      "branchedNames ['x2', 'x5', 'x8', 'x10', 'x11', 'x12', 'x13', 'x14']\n",
      "branched [2, 5, 8, 10, 11, 12, 13, 14]\n",
      "CPXPARAM_Read_DataCheck                          1\n",
      "Tried aggregator 1 time.\n",
      "LP Presolve eliminated 0 rows and 98 columns.\n",
      "Reduced LP has 85 rows, 532 columns, and 4829 nonzeros.\n",
      "Presolve time = 0.00 sec. (0.53 ticks)\n",
      "Initializing dual steep norms . . .\n",
      "\n",
      "Iteration log . . .\n",
      "Iteration:     1   Scaled dual infeas =           265.244964\n",
      "Iteration:    62   Scaled dual infeas =             1.505411\n",
      "Iteration:   122   Dual objective     =             0.057282\n",
      "Iteration:   183   Dual objective     =             0.025695\n",
      "Iteration:   245   Dual objective     =             0.000000\n",
      "Perturbation started.\n",
      "Iteration:   275   Dual objective     =             0.000000\n",
      "Removing perturbation.\n",
      "SecondModelOutput[1] 0.0\n",
      "SecondModelOutput[0] [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "self.times_called 14\n",
      "get_num_nodes 8\n",
      "values [1.0, 1.0, 1.0, -0.0, -0.0, 1.0, 1.0, 1.0]\n",
      "branchedNames ['x2', 'x5', 'x8', 'x10', 'x11', 'x12', 'x13', 'x14']\n",
      "branched [2, 5, 8, 10, 11, 12, 13, 14]\n",
      "self.times_called 15\n",
      "get_num_nodes 9\n",
      "values [1.0, -0.0, 1.0, 1.0, -0.0, -0.0, 1.0, 1.0, 1.0]\n",
      "branchedNames ['x2', 'x4', 'x5', 'x8', 'x10', 'x11', 'x12', 'x13', 'x14']\n",
      "branched [2, 4, 5, 8, 10, 11, 12, 13, 14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPXPARAM_Read_DataCheck                          1\n",
      "Tried aggregator 1 time.\n",
      "LP Presolve eliminated 0 rows and 72 columns.\n",
      "Reduced LP has 76 rows, 468 columns, and 4073 nonzeros.\n",
      "Presolve time = 0.00 sec. (0.45 ticks)\n",
      "Initializing dual steep norms . . .\n",
      "\n",
      "Iteration log . . .\n",
      "Iteration:     1   Scaled dual infeas =           250.831311\n",
      "Iteration:    62   Scaled dual infeas =             2.077426\n",
      "Iteration:   112   Dual objective     =             0.098193\n",
      "Iteration:   173   Dual objective     =             0.021568\n",
      "Iteration:   235   Dual objective     =             0.000000\n",
      "Perturbation started.\n",
      "Iteration:   265   Dual objective     =             0.000000\n",
      "Removing perturbation.\n",
      "SecondModelOutput[1] 0.0\n",
      "SecondModelOutput[0] [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011542720677058166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.379347422993627e-05, 4.830194433893557e-05, 1.4497148940434225e-05, 0.0, 0.0, 0.0, 0.0, 0.0001673605781978452, 0.0, 3.478977401031518e-05, 0.0, 0.0, 0.0, 2.0639986999929696e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005116703513997332, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012435327372769027, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009102677259382377, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006867434691213581, 0.0, 0.0, 0.013687728163526723, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009929518211280129, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014612394299594985, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.028258998512698714, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0017730346134699447, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0003030432839018106, 8.35717900280889e-05, 0.00042733506330440256, 0.0, 0.0, 0.0, 1.2168470987462836e-05, 0.0005062369120682923, 0.0, 7.493024818996588e-05, 0.0, 0.0, 8.648116208293797e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07118429741279099, 0.0, 0.04055437409503723, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014606727189257786, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014374029333556377, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03222989670782673, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03661554112808649, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00011899430786158938, 0.0, 1.0207680718062053e-05, 0.00024118642160738302, 0.0, 0.0, 0.0, 0.0, 0.00032645532647662814, 0.00010435447868232922, 6.969106263270407e-06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01641441428045072, 0.0, 0.0, 0.0, 0.14114423063332027, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02170322925425705, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.021339472196729894, 0.0, 0.0, 0.0, 0.05303104777463093, 0.0, 0.007232820698297028, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010126806206760037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0001132884831831776, 0.0, 0.0, 0.0, 0.0, 0.0002796036841849253, 0.0, 0.0, 0.0002953826320993986, 0.0, 0.0, 0.0006996181905949571, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.021041608792690115, 0.0, 0.0, 0.03438638131368466, 0.0, 0.0, 0.0, 0.0, 0.053002631290935714, 0.0, 0.0, 0.0, 0.0, 0.020622994031659615, 0.015784583253014278, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06595298464960266, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11107664916290369, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0462869074592079, 0.033942625671357224, 0.0, 0.0, 0.0]\n",
      "self.times_called 16\n",
      "get_num_nodes 9\n",
      "values [1.0, -0.0, 1.0, 1.0, -0.0, -0.0, 1.0, 1.0, 1.0]\n",
      "branchedNames ['x2', 'x4', 'x5', 'x8', 'x10', 'x11', 'x12', 'x13', 'x14']\n",
      "branched [2, 4, 5, 8, 10, 11, 12, 13, 14]\n",
      "self.times_called 17\n",
      "get_num_nodes 12\n",
      "values [1.0, 1.0, 1.0, 1.0, -0.0, 1.0, 1.0, 1.0]\n",
      "branchedNames ['x2', 'x5', 'x8', 'x10', 'x11', 'x12', 'x13', 'x14']\n",
      "branched [2, 5, 8, 10, 11, 12, 13, 14]\n",
      "CPXPARAM_Read_DataCheck                          1\n",
      "Tried aggregator 1 time.\n",
      "LP Presolve eliminated 0 rows and 98 columns.\n",
      "Reduced LP has 85 rows, 532 columns, and 4829 nonzeros.\n",
      "Presolve time = 0.00 sec. (0.53 ticks)\n",
      "Initializing dual steep norms . . .\n",
      "\n",
      "Iteration log . . .\n",
      "Iteration:     1   Scaled dual infeas =           631.999990\n",
      "Iteration:    62   Scaled dual infeas =             2.357975\n",
      "Iteration:   124   Scaled dual infeas =             0.172152\n",
      "Iteration:   156   Dual objective     =             0.092933\n",
      "Iteration:   217   Dual objective     =             0.017139\n",
      "Iteration:   279   Dual objective     =             0.008169\n",
      "Iteration:   341   Dual objective     =             0.003033\n",
      "SecondModelOutput[1] 0.00256479260593\n",
      "SecondModelOutput[0] [0.0, 0.0, 0.00011780487105296943, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.101141120944996e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0012995188468899051, 0.0, 0.0, 0.009453711806695485, 0.017564783441025418, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009829936696011017, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00851463580048704, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012701236784969181, 0.0, 0.004147706356599516, 0.0, 0.006530581622214699, 0.0, 0.004525052233320151, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.370713101290111e-06, 0.0, 0.0, 0.0, 0.0, 1.1725646909917392e-06, 2.1737194978981615e-06, 2.5365139049332514e-06, 0.0, 5.934358805192086e-07, 0.0, 1.2930301466574967e-06, 0.0, 2.0826523532614797e-06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00025628876682284033, 0.0, 0.008322630746382424, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04778668147824911, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0002599721255296726, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.733833267743183e-05, 0.0, 0.0, 0.0, 9.353791273172071e-06, 0.0, 2.672511792334878e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004269282870652194, 0.0, 0.0014908656524499232, 0.0, 0.0, 0.00014259995020335, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0010392432331046923, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003346875601267379, 0.0, 0.0035005615263130068, 0.0017119912577860756, 0.0010101434695570938, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.152041862411799e-05, 0.0, 0.00013558179935787578, 0.00022599377194237164, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01239877113203646, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010909646685532104, 0.0, 0.0045415323205254664, 0.013686944297644379, 0.014209258208426698, 0.0, 0.0, 0.0, 0.014584640627064365, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007036357137188402, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.025961612319598728, 0.0, 0.0, 0.004585320106688984, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0002725545101696413, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0001994251657372166, 0.0, 0.0001465709792683326, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.669132991252566e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010127441186782944, 0.0, 0.040045345587536016, 0.0, 0.0, 0.0, 0.017135662452661307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011176626964014485, 0.0, 0.0, 0.02831845500987869, 0.0, 0.0, 0.021765299694647963, 0.0, 0.0, 0.0, 0.0, 0.004909596366858978, 0.0011116083812943468, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04503137456163731, 0.0, 0.0, 0.0, 0.013507736422631538, 0.0, 0.0, 0.0, 0.0, 0.0003500802090056689, 0.0, 0.0005885803421319868, 0.0, 0.0, 0.0, 0.000572628816361494, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6415384095532362e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01997270378442164, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06904002537574211, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10332487676867443, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06449603037598269, 0.0, 0.0, 0.0, 0.0, 0.0684151369594655, 0.0, 0.017709785062283795, 0.03987930815366072, 0.07998248350296522, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004521329871582956, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.2677398365368186e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0176150727863984e-05, 0.0, 0.0, 0.000146876616610753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0026896440845070413, 0.0, 0.0, 0.007947340536652364, 0.0, 0.01013580072093742, 0.0, 0.0, 0.01225107653931827, 0.0, 0.0, 0.0, 0.0, 0.004620506794362318, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00724293493724273, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007762512493305994, 0.0, 0.016735414412819786, 0.0, 0.007319082686701654, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0041952481641739136, 0.0, 0.0, 0.0]\n",
      "FinalLhs [ 0.0285211   0.08837923  0.0116962   0.10696537 -0.03101642  0.05989467\n",
      "  0.06638392  0.13140701] FinalRhs [0.49068269]\n",
      "output [ 0.0285211   0.08837923  0.0116962   0.10696537 -0.03101642  0.05989467\n",
      "  0.06638392  0.13140701]\n",
      "output[1] [0.49068269]\n",
      "lhs SparsePair(ind = ['x2', 'x5', 'x8', 'x10', 'x11', 'x12', 'x13', 'x14'], val = array([ 0.0285211 ,  0.08837923,  0.0116962 ,  0.10696537, -0.03101642,\n",
      "        0.05989467,  0.06638392,  0.13140701]))\n",
      "cut added\n",
      "self.times_called 18\n",
      "get_num_nodes 14\n",
      "values [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -0.0]\n",
      "branchedNames ['x0', 'x2', 'x5', 'x8', 'x11', 'x12', 'x13', 'x14']\n",
      "branched [0, 2, 5, 8, 11, 12, 13, 14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPXPARAM_Read_DataCheck                          1\n",
      "Tried aggregator 1 time.\n",
      "LP Presolve eliminated 0 rows and 98 columns.\n",
      "Reduced LP has 85 rows, 532 columns, and 4829 nonzeros.\n",
      "Presolve time = 0.02 sec. (0.53 ticks)\n",
      "Initializing dual steep norms . . .\n",
      "\n",
      "Iteration log . . .\n",
      "Iteration:     1   Scaled dual infeas =           240.987801\n",
      "Iteration:    62   Scaled dual infeas =             2.236774\n",
      "Iteration:   124   Scaled dual infeas =             0.236591\n",
      "Iteration:   181   Dual objective     =             0.034513\n",
      "Iteration:   242   Dual objective     =             0.000000\n",
      "Perturbation started.\n",
      "Iteration:   283   Dual objective     =             0.000000\n",
      "Removing perturbation.\n",
      "SecondModelOutput[1] 0.0\n",
      "SecondModelOutput[0] [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "self.times_called 19\n",
      "get_num_nodes 14\n",
      "values [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -0.0]\n",
      "branchedNames ['x0', 'x2', 'x5', 'x8', 'x11', 'x12', 'x13', 'x14']\n",
      "branched [0, 2, 5, 8, 11, 12, 13, 14]\n",
      "self.times_called 20\n",
      "get_num_nodes 15\n",
      "values [1.0, 1.0, -0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -0.0]\n",
      "branchedNames ['x0', 'x2', 'x4', 'x5', 'x6', 'x8', 'x11', 'x12', 'x13', 'x14']\n",
      "branched [0, 2, 4, 5, 6, 8, 11, 12, 13, 14]\n",
      "CPXPARAM_Read_DataCheck                          1\n",
      "Tried aggregator 1 time.\n",
      "LP Presolve eliminated 0 rows and 50 columns.\n",
      "Reduced LP has 66 rows, 400 columns, and 3340 nonzeros.\n",
      "Presolve time = 0.00 sec. (0.37 ticks)\n",
      "Initializing dual steep norms . . .\n",
      "\n",
      "Iteration log . . .\n",
      "Iteration:     1   Scaled dual infeas =            15.000000\n",
      "Iteration:    62   Scaled dual infeas =             0.067775\n",
      "Iteration:    88   Dual objective     =             0.006203\n",
      "Iteration:   149   Dual objective     =             0.001787\n",
      "SecondModelOutput[1] 0.0\n",
      "SecondModelOutput[0] [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "self.times_called 21\n",
      "get_num_nodes 15\n",
      "values [1.0, 1.0, -0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -0.0]\n",
      "branchedNames ['x0', 'x2', 'x4', 'x5', 'x6', 'x8', 'x11', 'x12', 'x13', 'x14']\n",
      "branched [0, 2, 4, 5, 6, 8, 11, 12, 13, 14]\n",
      "MIP_optimal\n",
      "Objective value =  530.0\n",
      "\n",
      "Column 0: Value =                 1\n",
      "Column 1: Value =                 1\n",
      "Column 2: Value =                 1\n",
      "Column 5: Value =                 1\n",
      "Column 6: Value =                 1\n",
      "Column 7: Value =                 1\n",
      "Column 8: Value =                 1\n",
      "Column 9: Value =                 1\n",
      "Column 11: Value =                 1\n",
      "Column 12: Value =                 1\n",
      "Column 13: Value =                 1\n",
      "Branch callback was called  21 times\n"
     ]
    }
   ],
   "source": [
    "def admipex3(c):\n",
    "    \n",
    "    c.parameters.preprocessing.presolve = 0\n",
    "\n",
    "    c.parameters.preprocessing.reduce = 0\n",
    "\n",
    "    c.parameters.preprocessing.linear.set(0)\n",
    "\n",
    "    c.parameters.preprocessing.numpass = 0\n",
    "\n",
    "    c.parameters.mip.strategy.presolvenode = -1\n",
    "\n",
    "    c.parameters.preprocessing.linear = 0\n",
    "    \n",
    "def CutCplex(c):\n",
    "    c.parameters.mip.cuts.disjunctive.set(-1)\n",
    "    c.parameters.mip.cuts.bqp.set(-1)\n",
    "    c.parameters.mip.cuts.cliques.set(-1)\n",
    "    c.parameters.mip.cuts.covers.set(-1)\n",
    "    c.parameters.mip.cuts.flowcovers.set(-1)\n",
    "    c.parameters.mip.cuts.gomory.set(-1)\n",
    "    c.parameters.mip.cuts.gubcovers.set(-1)\n",
    "    c.parameters.mip.cuts.implied.set(-1)\n",
    "    c.parameters.mip.cuts.liftproj.set(-1)\n",
    "    c.parameters.mip.cuts.localimplied.set(-1)\n",
    "    c.parameters.mip.cuts.mcfcut.set(-1)\n",
    "    c.parameters.mip.cuts.mircut.set(-1)\n",
    "    c.parameters.mip.cuts.zerohalfcut.set(-1)\n",
    "    c.parameters.mip.cuts.pathcut.set(-1)\n",
    "    \n",
    "c = CPX.Cplex('NewExample.lp')\n",
    "m=c.linear_constraints.get_num()\n",
    "n=c.variables.get_num()\n",
    "my_rhs=[]\n",
    "my_coef=[]\n",
    "for i in range(m):\n",
    "    my_row=[]\n",
    "    for j in range(n):\n",
    "        my_row.append(c.linear_constraints.get_coefficients(i,j))\n",
    "    my_coef.append(my_row)\n",
    "\n",
    "for i in range (m):\n",
    "    my_rhs.append(c.linear_constraints.get_rhs(i))\n",
    "\n",
    "admipex3(c)\n",
    "CutCplex(c)\n",
    "VarsName=c.variables.get_names()\n",
    "cplexlog = \"cplex.log\"\n",
    "\n",
    "\n",
    "c.set_log_stream(sys.stdout)\n",
    "c.set_results_stream(sys.stdout)\n",
    "\n",
    "cutinst=c.register_callback(MyCut)\n",
    "cutinst.times_called = 0\n",
    "cutinst.m=m\n",
    "cutinst.n=n\n",
    "cutinst.my_rhs=my_rhs\n",
    "cutinst.my_coef=my_coef\n",
    "cutinst.VarsName=VarsName\n",
    "BranchedMatrix=[[[-1], [-0.0]]]\n",
    "c.parameters.mip.interval.set(1)\n",
    "c.parameters.preprocessing.linear=0\n",
    "c.parameters.mip.strategy.search.set(c.parameters.mip.strategy.search.values.traditional)\n",
    "c.solve()\n",
    "\n",
    "c.write(\"lpex1.lp\")\n",
    "solution = c.solution\n",
    "\n",
    "# the following line prints the corresponding string\n",
    "print solution.status[solution.get_status()]\n",
    "print \"Objective value = \" , solution.get_objective_value()\n",
    "print\n",
    "x = solution.get_values(0, c.variables.get_num()-1)\n",
    "for j in range(c.variables.get_num()):\n",
    "    if fabs(x[j]) > 1.0e-10:\n",
    "        print \"Column %d: Value = %17.10g\" % (j, x[j])\n",
    "\n",
    "print \"Branch callback was called \", cutinst.times_called, \"times\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
